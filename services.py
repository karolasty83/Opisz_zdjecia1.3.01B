# -*- coding: utf-8 -*-
# AUTOGENERATED SPLIT: services (sieć + worker)
import os, base64, requests, threading, wx, time
from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED
from constants import MAX_WORKERS, MODEL_NAME
from exif_io import add_description_to_jpg
from core import (
    get_provider_from_config,  # NOWE: sprawdzamy, kogo użyć ("openai" | "gemini")
    get_gpt_threads_from_config,
    get_gemini_rpm_from_config,
    get_gemini_batch_pause_from_config,
)

# -----------------------------------
# Konfiguracja modeli / endpointów
# -----------------------------------
# OpenAI – jak było
OPENAI_CHAT_COMPLETIONS_URL = "https://api.openai.com/v1/chat/completions"

# Gemini – domyślny model i endpoint (Vision/Multimodal)
# Jeśli wolisz inny model, zmień poniżej:
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_ENDPOINT_TMPL = (
    "https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
)

# -----------------------------------
# Worker – równoległe opisywanie
# -----------------------------------
def guess_mime(path):
    ext = os.path.splitext(path)[1].lower()
    if ext in (".jpg", ".jpeg"):
        return "image/jpeg"
    if ext == ".png":
        return "image/png"
    if ext == ".webp":
        return "image/webp"
    return "application/octet-stream"


def encode_image_to_base64_data_url(path):
    mime = guess_mime(path)
    with open(path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode("utf-8")
    return f"data:{mime};base64,{b64}"


def read_image_as_base64(path):
    """Surowa baza64 (bez data:) – potrzebne dla Gemini inline_data."""
    with open(path, "rb") as f:
        return base64.b64encode(f.read()).decode("utf-8")

def encode_bytes_to_data_url(image_bytes: bytes, mime: str) -> str:
    b64 = base64.b64encode(image_bytes).decode("utf-8")
    return f"data:{mime};base64,{b64}"

def encode_bytes_to_b64(image_bytes: bytes) -> str:
    return base64.b64encode(image_bytes).decode("utf-8")


class ImageDescriber(threading.Thread):
    def __init__(self, parent, api_key, image_files, prompt_text, emit_existing=True):
        super().__init__(daemon=True)
        self.parent = parent
        self.api_key = api_key
        self.image_files = image_files
        self.prompt_text = prompt_text
        self.emit_existing = emit_existing
        self._stop = threading.Event()
        self._executor = None

    def cancel(self):
        try:
            self._stop.set()
            ex = getattr(self, "_executor", None)
            if ex:
                # Anuluj zadania oczekujące (nie wystartowane) – bez blokowania UI
                ex.shutdown(cancel_futures=True, wait=False)
        except Exception:
            pass

    def run(self):
        total = len(self.image_files)
        done = 0

        provider = (get_provider_from_config() or "openai").lower()
        if provider == "gemini":
            # Partie zgodnie z limitami z configu
            per_batch = max(1, int(get_gemini_rpm_from_config()))
            pause_s = max(1, int(get_gemini_batch_pause_from_config()))

            i = 0
            while i < total and not self._stop.is_set():
                batch = self.image_files[i:i + per_batch]
                # równolegle w obrębie partii
                with ThreadPoolExecutor(max_workers=len(batch)) as executor:
                    futures = {executor.submit(self.describe_image, img): img for img in batch}
                    for future in futures:
                        if self._stop.is_set():
                            break
                    for future in futures:
                        if self._stop.is_set():
                            break
                        img = futures[future]
                        try:
                            desc = future.result()
                            done += 1
                            if img.lower().endswith((".jpg", ".jpeg")):
                                try:
                                    add_description_to_jpg(img, desc)
                                except Exception:
                                    pass
                            if self.parent and not getattr(self.parent, "canceled", False):
                                wx.CallAfter(self.parent.update_result, img, desc, done, total, True)
                        except Exception as e:
                            if self.parent and not getattr(self.parent, "canceled", False):
                                wx.CallAfter(self.parent.update_result, img, f"[BŁĄD] {e}", done, total, False)

                i += len(batch)
                if i < total and not self._stop.is_set():
                    # krótki sen z pętlą, aby reagować na cancel
                    slept = 0
                    step = 0.05
                    while slept < pause_s and not self._stop.is_set():
                        time.sleep(step)
                        slept += step

            if not self._stop.is_set() and self.parent and not getattr(self.parent, "canceled", False):
                wx.CallAfter(self.parent.on_all_done)
            return

        # OpenAI/GPT: liczba wątków z configu
        workers = max(1, int(get_gpt_threads_from_config()))
        with ThreadPoolExecutor(max_workers=workers) as executor:
            self._executor = executor
            futures = {executor.submit(self.describe_image, img): img for img in self.image_files}
            not_done = set(futures.keys())
            while not_done:
                if self._stop.is_set():
                    for f in list(not_done):
                        try:
                            f.cancel()
                        except Exception:
                            pass
                    break
                done_set, not_done = wait(not_done, timeout=0.05, return_when=FIRST_COMPLETED)
                for future in done_set:
                    img = futures.get(future)
                    if img is None:
                        continue
                    done += 1
                    try:
                        desc = future.result()
                        if img.lower().endswith((".jpg", ".jpeg")):
                            try:
                                add_description_to_jpg(img, desc)
                            except Exception:
                                pass
                        if self.parent and not getattr(self.parent, "canceled", False):
                            wx.CallAfter(self.parent.update_result, img, desc, done, total, True)
                    except Exception as e:
                        if self.parent and not getattr(self.parent, "canceled", False):
                            wx.CallAfter(self.parent.update_result, img, f"[BŁĄD] {e}", done, total, False)

        if not self._stop.is_set() and self.parent and not getattr(self.parent, "canceled", False):
            wx.CallAfter(self.parent.on_all_done)

    def describe_image(self, image_path):
        """Automatycznie wybiera provider na podstawie configu."""
        # Wczesne wyjście po anulowaniu
        if self._stop.is_set():
            raise RuntimeError("Anulowano")
        provider = (get_provider_from_config() or "openai").lower()
        if provider == "gemini":
            return self._describe_with_gemini(image_path)
        # domyślnie – OpenAI (zachowanie „po staremu”)
        return self._describe_with_openai(image_path)

    # ---------------- OpenAI ----------------
    def _describe_with_openai(self, image_path):
        url = OPENAI_CHAT_COMPLETIONS_URL
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        img_data_url = encode_image_to_base64_data_url(image_path)

        payload = {
            "model": MODEL_NAME,  # z constants.py (np. "gpt-5")
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": self.prompt_text},
                        {"type": "image_url", "image_url": {"url": img_data_url}},
                    ],
                }
            ],
        }

        resp = requests.post(url, headers=headers, json=payload, timeout=90)
        if resp.status_code != 200:
            raise RuntimeError(f"API error {resp.status_code}: {resp.text}")
        out = resp.json()
        if "error" in out:
            raise RuntimeError(f"API zwróciło błąd: {out['error']}")
        choices = out.get("choices")
        if not choices:
            raise RuntimeError(f"Odpowiedź bez 'choices': {out}")
        message = choices[0].get("message", {})
        content = message.get("content", "")
        if not isinstance(content, str) or not content.strip():
            raise RuntimeError(f"Brak treści odpowiedzi: {out}")
        return content.strip()

    # ---------------- Gemini ----------------
    def _describe_with_gemini(self, image_path):
        """Wyślij prośbę do Gemini 1.5 (Vision)."""
        mime = guess_mime(image_path)
        img_b64 = read_image_as_base64(image_path)

        url = GEMINI_ENDPOINT_TMPL.format(model=GEMINI_MODEL_NAME)
        params = {"key": self.api_key}
        headers = {"Content-Type": "application/json"}

        # Struktura żądania Gemini (generateContent)
        payload = {
            "contents": [
                {
                    "parts": [
                        {"text": self.prompt_text or "Describe the image."},
                        {
                            "inline_data": {
                                "mime_type": mime,
                                "data": img_b64
                            }
                        }
                    ]
                }
            ]
        }

        resp = requests.post(url, params=params, headers=headers, json=payload, timeout=90)
        if resp.status_code != 200:
            raise RuntimeError(f"Gemini error {resp.status_code}: {resp.text}")

        data = resp.json()
        prompt_feedback = data.get("promptFeedback") or {}
        block_reason = prompt_feedback.get("blockReason")

        candidates = data.get("candidates") or []
        if candidates:
            parts = (candidates[0].get("content") or {}).get("parts") or []
            texts = []
            for part in parts:
                text = part.get("text")
                if isinstance(text, str) and text.strip():
                    texts.append(text.strip())
            content = "\n".join(texts).strip()
            if content:
                return content
            if block_reason:
                return self._handle_gemini_block(image_path, block_reason)
            raise RuntimeError(f"Brak treści w odpowiedzi Gemini: {data}")

        if block_reason:
            return self._handle_gemini_block(image_path, block_reason)
        raise RuntimeError(f"Brak 'candidates' w odpowiedzi: {data}")

    def _handle_gemini_block(self, image_path: str, block_reason: str) -> str:
        try:
            return self._attempt_gemini_followup(image_path, block_reason)
        except Exception as err:
            raise RuntimeError(
                f"Gemini odrzuciło treść ({block_reason}). Próba dodatkowego pytania nie powiodła się: {err}"
            )

    def _attempt_gemini_followup(self, image_path: str, block_reason: str) -> str:
        prompt = self.prompt_text or "Describe the image."
        base_description = f"Automatyczny opis został wstrzymany przez Gemini (powód: {block_reason})."
        return _ask_followup_with_gemini(
            self.api_key,
            prompt,
            image_path,
            base_description,
            [],
            prompt,
        )

# -----------------------------------
# Bezpośrednie opisywanie danych w pamięci (kamera itp.)
# -----------------------------------
def describe_image_from_bytes(api_key: str, prompt_text: str, image_bytes: bytes, mime: str = "image/jpeg") -> str:
    prompt = prompt_text or "Opisz obraz."
    provider = (get_provider_from_config() or "openai").lower()
    if provider == "gemini":
        return _describe_bytes_with_gemini(api_key, prompt, image_bytes, mime)
    return _describe_bytes_with_openai(api_key, prompt, image_bytes, mime)

def _describe_bytes_with_openai(api_key: str, prompt_text: str, image_bytes: bytes, mime: str) -> str:
    url = OPENAI_CHAT_COMPLETIONS_URL
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    img_data_url = encode_bytes_to_data_url(image_bytes, mime)
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt_text},
                    {"type": "image_url", "image_url": {"url": img_data_url}},
                ],
            }
        ],
    }
    resp = requests.post(url, headers=headers, json=payload, timeout=90)
    if resp.status_code != 200:
        raise RuntimeError(f"API error {resp.status_code}: {resp.text}")
    out = resp.json()
    if "error" in out:
        raise RuntimeError(f"API zwróciło błąd: {out['error']}")
    choices = out.get("choices")
    if not choices:
        raise RuntimeError(f"Odpowiedź bez 'choices': {out}")
    message = choices[0].get("message", {})
    content = message.get("content", "")
    if not isinstance(content, str) or not content.strip():
        raise RuntimeError(f"Brak treści odpowiedzi: {out}")
        return content.strip()

def _describe_bytes_with_gemini(api_key: str, prompt_text: str, image_bytes: bytes, mime: str) -> str:
    url = GEMINI_ENDPOINT_TMPL.format(model=GEMINI_MODEL_NAME)
    params = {"key": api_key}
    headers = {"Content-Type": "application/json"}
    img_b64 = encode_bytes_to_b64(image_bytes)
    payload = {
        "contents": [
            {
                "parts": [
                    {"text": prompt_text or "Describe the image."},
                    {
                        "inline_data": {
                            "mime_type": mime,
                            "data": img_b64
                        }
                    }
                ]
            }
        ]
    }
    resp = requests.post(url, params=params, headers=headers, json=payload, timeout=90)
    if resp.status_code != 200:
        raise RuntimeError(f"Gemini error {resp.status_code}: {resp.text}")
    data = resp.json()
    prompt_feedback = data.get("promptFeedback") or {}
    block_reason = prompt_feedback.get("blockReason")

    candidates = data.get("candidates") or []
    if candidates:
        parts = (candidates[0].get("content") or {}).get("parts") or []
        texts = []
        for part in parts:
            text = part.get("text")
            if isinstance(text, str) and text.strip():
                texts.append(text.strip())
        content = "\n".join(texts).strip()
        if content:
            return content
        if block_reason:
            return _handle_gemini_block_bytes(api_key, prompt_text, image_bytes, mime, block_reason)
        raise RuntimeError(f"Brak treści w odpowiedzi Gemini: {data}")

    if block_reason:
        return _handle_gemini_block_bytes(api_key, prompt_text, image_bytes, mime, block_reason)
    raise RuntimeError(f"Brak 'candidates' w odpowiedzi: {data}")

def _handle_gemini_block_bytes(
    api_key: str,
    prompt_text: str,
    image_bytes: bytes,
    mime: str,
    block_reason: str,
) -> str:
    try:
        return _attempt_gemini_followup_bytes(
            api_key,
            prompt_text,
            image_bytes,
            mime,
            block_reason,
        )
    except Exception as err:
        raise RuntimeError(
            f"Gemini odrzuciło treść ({block_reason}). Próba dodatkowego pytania nie powiodła się: {err}"
        )


def _attempt_gemini_followup_bytes(
    api_key: str,
    prompt_text: str,
    image_bytes: bytes,
    mime: str,
    block_reason: str,
) -> str:
    prompt = prompt_text or "Describe the image."
    base_description = f"Automatyczny opis został wstrzymany przez Gemini (powód: {block_reason})."
    return _ask_followup_with_gemini_bytes(
        api_key,
        prompt,
        image_bytes,
        mime,
        base_description,
        [],
        prompt,
    )


# -----------------------------------
# Pytania uzupełniające do opisów
# -----------------------------------
def ask_followup_question(
    api_key: str,
    prompt_text: str,
    image_path: str,
    base_description: str,
    history_messages,
    question: str,
) -> str:
    """
    Zadaje dodatkowe pytanie o obraz, przekazując pełen kontekst (obraz + dotychczasowe wiadomości).

    history_messages – iterowalny zbiór elementów {"role": "user"|"assistant", "text": "..."}.
    """
    if not question or not str(question).strip():
        raise RuntimeError("Pytanie nie może być puste.")

    prompt = prompt_text or "Opisz obraz."
    provider = (get_provider_from_config() or "openai").lower()
    history = list(history_messages or [])
    if provider == "gemini":
        return _ask_followup_with_gemini(api_key, prompt, image_path, base_description, history, question)
    return _ask_followup_with_openai(api_key, prompt, image_path, base_description, history, question)


def ask_followup_question_from_bytes(
    api_key: str,
    prompt_text: str,
    image_bytes: bytes,
    mime: str,
    base_description: str,
    history_messages,
    question: str,
) -> str:
    """
    Wersja pytania uzupełniającego dla obrazów dostępnych w pamięci.
    """
    if not question or not str(question).strip():
        raise RuntimeError("Pytanie nie może być puste.")
    prompt = prompt_text or "Opisz obraz."
    provider = (get_provider_from_config() or "openai").lower()
    history = list(history_messages or [])
    mime = mime or "image/jpeg"
    if provider == "gemini":
        return _ask_followup_with_gemini_bytes(api_key, prompt, image_bytes, mime, base_description, history, question)
    return _ask_followup_with_openai_bytes(api_key, prompt, image_bytes, mime, base_description, history, question)


def _ask_followup_with_openai(
    api_key: str,
    prompt_text: str,
    image_path: str,
    base_description: str,
    history,
    question: str,
) -> str:
    url = OPENAI_CHAT_COMPLETIONS_URL
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    img_data_url = encode_image_to_base64_data_url(image_path)

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt_text},
                {"type": "image_url", "image_url": {"url": img_data_url}},
            ],
        }
    ]

    base_desc = (base_description or "").strip()
    if base_desc:
        messages.append({
            "role": "assistant",
            "content": [{"type": "text", "text": base_desc}],
        })

    for item in history:
        try:
            text = (item or {}).get("text", "").strip()
            role = (item or {}).get("role", "").strip().lower()
        except Exception:
            continue
        if not text:
            continue
        msg_role = "assistant" if role == "assistant" else "user"
        messages.append({
            "role": msg_role,
            "content": [{"type": "text", "text": text}],
        })

    messages.append({
        "role": "user",
        "content": [{"type": "text", "text": question}],
    })

    payload = {
        "model": MODEL_NAME,
        "messages": messages,
    }

    resp = requests.post(url, headers=headers, json=payload, timeout=90)
    if resp.status_code != 200:
        raise RuntimeError(f"API error {resp.status_code}: {resp.text}")
    out = resp.json()
    if "error" in out:
        raise RuntimeError(f"API zwróciło błąd: {out['error']}")
    choices = out.get("choices")
    if not choices:
        raise RuntimeError(f"Odpowiedź bez 'choices': {out}")
    message = choices[0].get("message", {})
    content = message.get("content", "")
    if not isinstance(content, str) or not content.strip():
        raise RuntimeError(f"Brak treści odpowiedzi: {out}")
    return content.strip()


def _ask_followup_with_openai_bytes(
    api_key: str,
    prompt_text: str,
    image_bytes: bytes,
    mime: str,
    base_description: str,
    history,
    question: str,
) -> str:
    url = OPENAI_CHAT_COMPLETIONS_URL
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    img_data_url = encode_bytes_to_data_url(image_bytes, mime)

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt_text},
                {"type": "image_url", "image_url": {"url": img_data_url}},
            ],
        }
    ]

    base_desc = (base_description or "").strip()
    if base_desc:
        messages.append({
            "role": "assistant",
            "content": [{"type": "text", "text": base_desc}],
        })

    for item in history:
        try:
            text = (item or {}).get("text", "").strip()
            role = (item or {}).get("role", "").strip().lower()
        except Exception:
            continue
        if not text:
            continue
        msg_role = "assistant" if role == "assistant" else "user"
        messages.append({
            "role": msg_role,
            "content": [{"type": "text", "text": text}],
        })

    messages.append({
        "role": "user",
        "content": [{"type": "text", "text": question}],
    })

    payload = {
        "model": MODEL_NAME,
        "messages": messages,
    }

    resp = requests.post(url, headers=headers, json=payload, timeout=90)
    if resp.status_code != 200:
        raise RuntimeError(f"API error {resp.status_code}: {resp.text}")
    out = resp.json()
    if "error" in out:
        raise RuntimeError(f"API zwróciło błąd: {out['error']}")
    choices = out.get("choices")
    if not choices:
        raise RuntimeError(f"Odpowiedź bez 'choices': {out}")
    message = choices[0].get("message", {})
    content = message.get("content", "")
    if not isinstance(content, str) or not content.strip():
        raise RuntimeError(f"Brak treści odpowiedzi: {out}")
    return content.strip()


def _ask_followup_with_gemini(
    api_key: str,
    prompt_text: str,
    image_path: str,
    base_description: str,
    history,
    question: str,
) -> str:
    mime = guess_mime(image_path)
    img_b64 = read_image_as_base64(image_path)

    url = GEMINI_ENDPOINT_TMPL.format(model=GEMINI_MODEL_NAME)
    params = {"key": api_key}
    headers = {"Content-Type": "application/json"}

    contents = [
        {
            "role": "user",
            "parts": [
                {"text": prompt_text or "Describe the image."},
                {
                    "inline_data": {
                        "mime_type": mime,
                        "data": img_b64
                    }
                }
            ]
        }
    ]

    base_desc = (base_description or "").strip()
    if base_desc:
        contents.append({
            "role": "model",
            "parts": [{"text": base_desc}],
        })

    for item in history:
        try:
            text = (item or {}).get("text", "").strip()
            role = (item or {}).get("role", "").strip().lower()
        except Exception:
            continue
        if not text:
            continue
        role_name = "model" if role == "assistant" else "user"
        contents.append({
            "role": role_name,
            "parts": [{"text": text}],
        })

    contents.append({
        "role": "user",
        "parts": [{"text": question}],
    })

    payload = {"contents": contents}

    resp = requests.post(url, params=params, headers=headers, json=payload, timeout=90)
    if resp.status_code != 200:
        raise RuntimeError(f"Gemini error {resp.status_code}: {resp.text}")

    data = resp.json()
    prompt_feedback = data.get("promptFeedback") or {}
    block_reason = prompt_feedback.get("blockReason")

    candidates = data.get("candidates") or []
    if candidates:
        parts = (candidates[0].get("content") or {}).get("parts") or []
        texts = []
        for part in parts:
            text = part.get("text")
            if isinstance(text, str) and text.strip():
                texts.append(text.strip())
        content = "\n".join(texts).strip()
        if content:
            return content
        if block_reason:
            return _handle_gemini_block_bytes(api_key, prompt_text, image_bytes, mime, block_reason)
        raise RuntimeError(f"Brak treści w odpowiedzi Gemini: {data}")

    if block_reason:
        return _handle_gemini_block_bytes(api_key, prompt_text, image_bytes, mime, block_reason)
    raise RuntimeError(f"Brak 'candidates' w odpowiedzi: {data}")


def _ask_followup_with_gemini_bytes(
    api_key: str,
    prompt_text: str,
    image_bytes: bytes,
    mime: str,
    base_description: str,
    history,
    question: str,
) -> str:
    mime = mime or "image/jpeg"
    img_b64 = encode_bytes_to_b64(image_bytes)

    url = GEMINI_ENDPOINT_TMPL.format(model=GEMINI_MODEL_NAME)
    params = {"key": api_key}
    headers = {"Content-Type": "application/json"}

    contents = [
        {
            "role": "user",
            "parts": [
                {"text": prompt_text or "Describe the image."},
                {
                    "inline_data": {
                        "mime_type": mime,
                        "data": img_b64
                    }
                }
            ]
        }
    ]

    base_desc = (base_description or "").strip()
    if base_desc:
        contents.append({
            "role": "model",
            "parts": [{"text": base_desc}],
        })

    for item in history:
        try:
            text = (item or {}).get("text", "").strip()
            role = (item or {}).get("role", "").strip().lower()
        except Exception:
            continue
        if not text:
            continue
        role_name = "model" if role == "assistant" else "user"
        contents.append({
            "role": role_name,
            "parts": [{"text": text}],
        })

    contents.append({
        "role": "user",
        "parts": [{"text": question}],
    })

    payload = {"contents": contents}

    resp = requests.post(url, params=params, headers=headers, json=payload, timeout=90)
    if resp.status_code != 200:
        raise RuntimeError(f"Gemini error {resp.status_code}: {resp.text}")

    data = resp.json()
    try:
        candidates = data.get("candidates") or []
        if not candidates:
            raise RuntimeError(f"Brak 'candidates' w odpowiedzi: {data}")
        parts = candidates[0]["content"].get("parts") or []
        texts = []
        for p in parts:
            t = p.get("text")
            if isinstance(t, str) and t.strip():
                texts.append(t.strip())
        content = "\n".join(texts).strip()
        if not content:
            raise RuntimeError(f"Brak treści w odpowiedzi Gemini: {data}")
        return content
    except Exception as e:
        raise RuntimeError(f"Niepoprawna odpowiedź Gemini: {e}")
